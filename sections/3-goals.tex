% !TEX root = ../main.tex
\section{Goals}

In this section, we define the objectives of this thesis, focusing on
addressing the limitations of existing datasets for code review automation. 
%Our primary goal is to construct a benchmark dataset that enables accurate evaluation of DL-based code review tools, focusing on two key tasks:  \textit{code quality check} and \textit{code refinement}.
The overall time expected 
%for the benchmark construction and the platform development 
is around 12 weeks; while, the expected time for each single goal is indicated in the title of its dedicated subsection.

% The time overall time we expect to spend of the development of the benchmark and the platform is around 12 weeks, in order to keep the last two weeks for the writing of the thesis. 
% There might be some parallel work from week 10 to 12, where we would both work development of the platform and start writing the thesis.

\subsection{Goal I: Benchmark for Quality Inspection Task [W1 - W4]}

The first goal of this work is to create a benchmark that evaluates DL models'
ability to assess code quality. This involves generating natural language
comments that emulate those written by human reviewers, identifying potential
code issues and recommending improvements. The dataset will be curated to
ensure the following:
\begin{itemize}
    \item Each triplet (\subCode, \revComment, \revCode) includes comments directly
          linked to a specific line of code within a method.
    \item The comments must be meaningful, requesting modifications relevant to the
          initial code. To ensure actual relevance, we will leverage ChatGPT's capabilities 
          to perform an initial check alongside manual analysis.
    \item Stability of modifications is ensured by confirming no subsequent edits to the
          affected lines within a specified period.
\end{itemize}

% Additionally, comments will be classified based on the contextual scope required
% for their resolution, ranging from method-level to project-level contexts. This
% classification ensures that the benchmark covers a diverse range of scenarios.

We will also generate paraphrases of the \revComment using ChatGPT, to include 
as correct those variants in the language that still retain relevance.

\subsection{Goal II: Benchmark for Code Refinement Task [W5 - W7]}
The second goal is to establish a benchmark for evaluating DL models' ability
to refine code based on reviewer comments. 
%The refinement task assesses how accurately a model can generate the revised code addressing the review comment.
Key requirements include:
\begin{itemize}
    % \item Validation of the alignment between the reviewerâ€™s comment and the implemented
          % changes, ensuring semantic equivalence where alternative implementations exist.
          % This again will be filtered out primarily through ChatGPT, with a subsequent
          % manual analysis to confirm the relevance of the code changes.
    \item Each modified code (\revCode) implements the associated comment, and at the same time does not introduce any further unrelated changes.
    \item Execution of tests to verify the functional correctness of the refined code.
\end{itemize}

The goal is to ensure that the prediction of the model aligns with what the
reviewer originally intended. The use of the tests is a key part, to allow the model to 
generate a different implementation than the actual one \revCode,
while still being semantically equivalent.

\subsection{Goal III: Empirical Study Comparing DL-based Code Review Automation
    Techniques [W7 - W9]}
We will perform a comprehensive empirical study to evaluate various 
state-of-the-art code review tools on the proposed benchmark measuring performance using several metrics % such as:
% \begin{itemize}
 %    \item Accuracy of comment generation and refinement predictions.
 %   \item Consistency in addressing diverse comment paraphrases and varying contexts.
 %   \item Efficiency and scalability in handling large-scale codebases.
% \end{itemize}
The aim is to compare the resulting performances of these tools when 
evaluated on noisy data, such as those used in the literature, and on clean data, thus revealing their real capabilities.

\subsection{Platform Development [W10 - W12]}

Finally, this thesis will include the development
of a web-based platform designed to evaluate the performance of models on
the proposed dataset. The platform will simplify the benchmarking process by allowing researchers to evaluate the performance of their approaches on the constructed dataset, simply by uploading predictions to the platform.
For the comment generation task, in case the model prediction differs from the actual \revComment but is similar to one of the generated paraphrases, we would also return the latter.

% The user will submit a CSV file containing three columns:

% \begin{enumerate}
 %    \item \texttt{problem$\_$id}: the id of the problem model is trying to solve
 %          (comment generation or code refinement)
 %    \item \texttt{instance$\_$id}: the instance of the benchmark
 %    \item \texttt{prediction}: the prediction of the model
% \end{enumerate}

Through these goals, this thesis seeks to establish a robust foundation for
evaluating and advancing DL-based code review tools, addressing key gaps in
current methodologies.

% !TEX root = ../main.tex
\section{CRAB: Code Review Automated Benchmark}

\subsection{Overview}

In this section, we describe the construction of a high-quality Java-only dataset of code-review
triplets. Each triplet comprises three elements: the code snapshot before the review, the natural
language review comment and the revised code after that change has been applied. Our work supports
two complementary benchmarks: the first is review comment generation, asks a model to predict the
review comment given the pre-review code; the second is code refinement, a strict subset of the
first, asks a model to generate the revised code that implements the suggestion.


We focused on Java projects and aimed for high quality over quantity. That means we only kept
examples that are self-contained, testable, and likely to be meaningful. As a starting point, we
used the \textit{GHS} dataset and tool introduced by Dabić \etal~\cite{Dabic:msr2021data}, which
supports project sampling for Mining Software Repositories (MSR) studies by providing rich metadata
for over 700,000 GitHub repositories. Their platform made it possible to filter for Java
repositories with relevant properties like number of commits, license, and recent activity.

This section explains how we constructed the dataset, how we defined the two benchmark tasks
(comment generation and code refinement), and how we filtered and validated the data to ensure it’s
useful for evaluating future models.


\subsection{Data Source}

\subsubsection{Repository Sampling}

We began by querying the \textit{GHS} dataset~\cite{Dabic:msr2021data}, a large-scale repository
mining platform that indexes metadata for over 700,000 GitHub projects. Using the web interface at
\url{https://seart-ghs.si.usi.ch}, we applied two simple filters:

\begin{itemize}
	\item The repository must have had at least 100 pull requests.
	\item The date of the last commit must fall within the year 2025.
\end{itemize}

These criteria ensured that we targeted active, collaborative projects likely to have meaningful
code review activity. We exported the matching repository list in CSV format and used it as input
for our processing pipeline.

We applied minimal filtering beyond that. A small number of repositories were manually excluded
during processing, mostly due to extreme build times or authentication issues.

\subsubsection{Pull Request Selection}

For each selected repository, we searched for pull requests that reflect a clean and minimal review
interaction. Specifically, we looked for PRs that:

\begin{itemize}
	\item Were successfully merged into the main branch.
	\item Contain either one review comment or two, where the second is a reply from the PR author.
\end{itemize}

The goal is to capture cases where a reviewer leaves a single comment and the author either does not
respond in writing, or replies directly within the thread — which we interpret as an acknowledgment
of the feedback. This keeps the setup simple and avoids ambiguous review discussions.

We made this design choice because we realized early on that it would be difficult to create clean
triplets from pull requests with multiple comments. While we can always locate where each comment
appears in the code, it's much harder to untangle which subsequent code changes correspond to which
comment in an automatic way. This makes it challenging to extract multiple, independent \subCode,
\revComment and \revCode triplets from a single pull request. By focusing on PRs with only one
comment (plus an optional reply), we ensure that each PR maps to a single, self-contained instance.
This keeps the data interpretable and makes the benchmark more reliable.

Of course, even in these single-comment PRs, not all follow-up commits are guaranteed to directly
address the reviewer’s feedback. We handle these exceptions through manual validation, as detailed
later in Section~\ref{sec:manual-selection}.

\subsection{Automated Extraction Pipeline}
\subsection{Build, Test \& Coverage}
\subsection{Manual selection}
\label{sec:manual-selection}
\subsection{Dataset Schema \& Serialization}
\subsection{Dataset Statistics}
\subsection{Summary \& Deliverables}

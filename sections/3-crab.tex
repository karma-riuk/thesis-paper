% !TEX root = ../main.tex
\section{CRAB: Code Review Automated Benchmark}

\subsection{Overview}

In this section, we describe the construction of a high-quality Java-only dataset of code-review
triplets. Each triplet comprises three elements: the code snapshot before the review, the natural
language review comment and the revised code after that change has been applied. Our work supports
two complementary benchmarks: the first is review comment generation, asks a model to predict the
review comment given the pre-review code; the second is code refinement, a strict subset of the
first, asks a model to generate the revised code that implements the suggestion.


We focused on Java projects and aimed for high quality over quantity. That means we only kept
examples that are self-contained, testable, and likely to be meaningful. As a starting point, we
used the \textit{GHS} dataset and tool introduced by Dabić \etal~\cite{Dabic:msr2021data}, which
supports project sampling for Mining Software Repositories (MSR) studies by providing rich metadata
for over 700,000 GitHub repositories. Their platform made it possible to filter for Java
repositories with relevant properties like number of commits, license, and recent activity.

This section explains how we constructed the dataset, how we defined the two benchmark tasks
(comment generation and code refinement), and how we filtered and validated the data to ensure it’s
useful for evaluating future models.


\subsection{Data Source}

\subsubsection{Repository Sampling}

We began by querying the \textit{GHS} dataset~\cite{Dabic:msr2021data}, a large-scale repository
mining platform that indexes metadata for over 700,000 GitHub projects. Using the web interface at
\url{https://seart-ghs.si.usi.ch}, we applied two simple filters:

\begin{itemize}
	\item The repository must have had at least 100 pull requests.
	\item The date of the last commit must fall within the year 2025.
\end{itemize}

These criteria ensured that we targeted active, collaborative projects likely to have meaningful
code review activity. We exported the matching repository list in CSV format and used it as input
for our processing pipeline.

We applied minimal filtering beyond that. A small number of repositories were manually excluded
during processing, mostly due to extreme build times or authentication issues.

\subsubsection{Pull Request Selection}

For each selected repository, we searched for pull requests that reflect a clean and minimal review
interaction. Specifically, we looked for PRs that:

\begin{itemize}
	\item Were successfully merged into the main branch.
	\item Contain either one review comment or two, where the second is a reply from the PR author.
\end{itemize}

The goal is to capture cases where a reviewer leaves a single comment and the author either does not
respond in writing, or replies directly within the thread — which we interpret as an acknowledgment
of the feedback. This keeps the setup simple and avoids ambiguous review discussions.

We made this design choice because we realized early on that it would be difficult to create clean
triplets from pull requests with multiple comments. While we can always locate where each comment
appears in the code, it's much harder to untangle which subsequent code changes correspond to which
comment in an automatic way. This makes it challenging to extract multiple, independent \subCode,
\revComment and \revCode triplets from a single pull request. By focusing on PRs with only one
comment (plus an optional reply), we ensure that each PR maps to a single, self-contained instance.
This keeps the data interpretable and makes the benchmark more reliable.

Of course, even in these single-comment PRs, not all follow-up commits are guaranteed to directly
address the reviewer’s feedback. We handle these exceptions through manual validation, as detailed
later in Section~\ref{sec:manual-selection}.

\subsection{Automated Extraction Pipeline}

To process the selected repositories and extract clean code-review triplets, we built a fully
automated pipeline that runs each pull request through a series of steps. The pipeline handles
repository cloning, commit checkout, diff extraction, file inspection, compilation, testing, and
coverage reporting — all inside isolated Docker containers.

Each pull request is treated independently and processed end-to-end. At every step, we log metadata
and errors to track why a PR was accepted or discarded. Figure~\ref{fig:pipeline} gives an overview
of this process. The full list of steps is detailed below, following the same numbering as in the
pipeline diagram.


\begin{figure}[!htbp]
	\centering
	\begin{tikzpicture}[
			node distance=1cm and 1cm,
			every node/.style={font=\small},
			startstop/.style={circle, draw, minimum size=1cm},
			process/.style={rectangle, draw, rounded corners, minimum width=6.5cm, align=center},
			decision/.style={diamond, draw, aspect=2, inner sep=1pt, align=center},
			arrow/.style={->, thick}
		]

		\node (start) [startstop] {Start};
		\node (diffs) [process, below of=start] {1. Extract diffs (before \& after)};
		\node (clone) [process, below of=diffs] {2. Clone repository};
		\node (files) [process, below of=clone] {3. Extract source files (before/after)};
		\node (comment) [process, below of=files] {4. Extract review comment};
		\node (checkout1) [process, below of=comment] {5. Checkout base commit};
		\node (archive1) [process, below of=checkout1] {6. Archive base snapshot};
		\node (checkout2) [process, below of=archive1] {7. Checkout merged commit};
		\node (archive2) [process, below of=checkout2] {8. Archive merged snapshot};

		\node (iscode) [decision, below of=archive2, yshift=-.6cm] {9. Is comment\\code-related?};

		% YES path
		\node (detect) [process, right of=iscode, xshift=7cm] {10. Detect build system (Maven/Gradle)};
		\node (build) [process, below of=detect] {11. Run build in Docker container};
		\node (test) [process, below of=build] {12. Run tests \& extract test results};
		\node (coverage) [process, below of=test] {13. Generate JaCoCo coverage reports};
		\node (record) [process, below of=coverage] {14. Record per-file coverage};

		% compute the midpoint between iscode and record
		\coordinate (mid) at ($ (iscode)!0.5!(record) $);
		\node (save) [process] at ($(mid |- record.south)+(0,-1cm)$) {15. Save entry};

		% arrows - linear path
		\draw[arrow] (start) -- (diffs);
		\draw[arrow] (diffs) -- (clone);
		\draw[arrow] (clone) -- (files);
		\draw[arrow] (files) -- (comment);
		\draw[arrow] (comment) -- (checkout1);
		\draw[arrow] (checkout1) -- (archive1);
		\draw[arrow] (archive1) -- (checkout2);
		\draw[arrow] (checkout2) -- (archive2);
		\draw[arrow] (archive2) -- (iscode);

		% branch arrows
		\draw[arrow] (iscode) -- (detect) node [pos=0.20, above] {yes};
		\draw[arrow] (detect) -- (build);
		\draw[arrow] (build) -- (test);
		\draw[arrow] (test) -- (coverage);
		\draw[arrow] (coverage) -- (record);

		% then re-draw your arrows:
		\draw[arrow] (record) |- (save);
		\draw[arrow] (iscode) |- (save) node [pos=0.025, left] {no};

		\node (end) [startstop, below of = save] {End};
		\draw[arrow] (save) -- (end);

	\end{tikzpicture}
	\caption{Automated pipeline for processing a single pull request.}
	\label{fig:pipeline}
\end{figure}

\begin{enumerate}
    \item \textbf{Extract diffs (before \& after).} Using the GitHub API, we compute the unified
        diffs between the base commit and the first comment, and  the first comment and the
        merged commit.

    \item \textbf{Clone repository.} The GitHub repository is cloned locally, with a check to avoid
        redundant downloads.

    \item \textbf{Extract source files (before/after).} We retrieve the full content of each changed
        file both before and after the merge, and store it in the \path{FileData} structure. If
        GitHub's API fails to deliver, we fall back to reading the file from disk.

    \item \textbf{Extract review comment.} We extract all review comments attached to the pull
        request and filter out those that do not point to a specific file and line range. The GitHub
        API can be inconsistent when it comes to providing line-level comment metadata, which is a
        known issue. To avoid uncertainty and reduce the amount of manual inspection required later,
        we exclude any comments that are missing this information.

    \item \textbf{Checkout base commit.} We check out the commit at the tip of the base branch
        before the pull request was opened. This gives us the clean version of the code before any
        of the proposed changes were introduced.

    \item \textbf{Archive base snapshot.} This version of the code is archived, creating a
        reproducible “before” state for future testing or manual inspection. It can also be used as
        context for a model that requires it.

    \item \textbf{Checkout merged commit.} We then check out the final merged commit of the pull
        request. This is the “after” state, assumed to reflect the author’s response to the review.

    \item \textbf{Archive merged snapshot.} Similar to step 3, we archive the post-merge state to
        preserve the final version of the code.

    \item \textbf{Is comment code-related?} We check whether the comment refers to a Java file. If
        not, there's no point running tests or generating coverage, so we skip straight to storage.

    \item \textbf{Detect build system (Maven/Gradle).} We scan the repository for common build files
        to determine whether to use Maven or Gradle. A specialized handler is then instantiated to
        run the remaining build steps inside Docker.

    \item \textbf{Run build in Docker container.} The project is compiled inside an isolated
        environment.

    \item \textbf{Run tests \& extract test results.} If compilation succeeds, we run the test suite
        and record whether the tests pass cleanly.

    \item \textbf{Generate JaCoCo coverage reports.} We inject a JaCoCo plugin (if necessary) and
        generate line-level test coverage data for each file.

    \item \textbf{Record per-file coverage.} We extract the percentage of covered lines for each
        file touched by the comment and store it in the dataset entry.

    \item \textbf{Save entry.} We save the pull request.
\end{enumerate}

Every pull request that meets our structural criteria (an initial set of commits, a single review
comment, and at least one follow-up commit) is saved to the dataset, regardless of whether the
pipeline completes successfully. Each processing step is allowed to fail independently, and failures
are handled through exception catching. This approach fits our use case well: when a step fails, it
usually means that continuing further would not yield usable results. Rather than discarding these
entries, we record the failure reason directly in the metadata. This makes it possible to gather
statistics about common failure points in the pipeline and refine the system over time. Importantly,
we chose not to remove these PRs entirely because they may still include high-quality review
comments, which can be useful for the comment generation task even if the code does not compile or
pass its tests.


\subsection{Build, Test \& Coverage}

Once valid pull requests have been identified and archived, the next step in the pipeline is to
analyze whether the code they modify is testable and whether it has sufficient test coverage. This
phase is essential not only for verifying that the pull request builds and functions as expected,
but also for enabling more meaningful and flexible evaluation in the code refinement task. Previous
approaches to evaluating model-generated code often relied on static similarity metrics such as
CodeBLEU or exact match, which reward syntactic closeness rather than semantic equivalence. This is
overly restrictive, as many code changes can be expressed in multiple correct and idiomatic ways
that differ significantly in surface form.

By ensuring that the code is testable and accompanied by meaningful test coverage, we enable an
alternative evaluation method based on dynamic behavior rather than static comparison. Specifically,
a model may generate a change in response to a reviewer comment, and the resulting code can be
tested to confirm that it does not introduce regressions. While the absence of test failures does
not guarantee the absolute correctness of a change, it does offer a strong signal that the
modification is not invalid. In contrast, a failing test definitively signals that the generated
code is incorrect.

This framing shifts the focus from syntactic conformity to functional acceptability, granting models
the freedom to propose diverse solutions as long as they preserve behavioral correctness. In
practice, this allows for a more robust and realistic assessment of machine learning systems
designed to respond to refine code. It also broadens the scope of acceptable outputs, better
reflecting the nature of real-world software development where stylistic preferences and
implementation choices can vary without violating correctness.

The first task is to determine whether the repository contains any tests at all. This is achieved by
analyzing the repository’s build configuration file, which is typically either a \texttt{pom.xml}
file for Maven-based projects or a \texttt{build.gradle} file for Gradle-based ones. The detection
process involves scanning for references to well-known testing frameworks such as JUnit, TestNG, or
Mockito, as well as keywords and patterns commonly associated with test declarations, such as
\texttt{testImplementation} or \texttt{functionalTests}. In addition to build file analysis, the
directory structure of the project is also inspected for conventional test folders, such as
\texttt{src/test/java} or \texttt{test/}, which frequently indicate the presence of test cases even
if the build file is not explicit.

If the project appears to contain test logic, it is then compiled and tested inside a controlled
environment. To ensure reproducibility, isolation, and a uniform execution environment across all
repositories, all build and test operations are performed inside Docker containers. There are two
container types, one for Maven and one for Gradle, each preconfigured with the appropriate tools and
dependencies needed to execute a typical Java project using the corresponding build system. These
containers encapsulate everything necessary to build and test a repository, and they help avoid
issues arising from environmental differences, such as missing dependencies, incompatible Java
versions, or local configuration conflicts. 

The architecture is deliberately modular and extensible. Each build system is handled by its own
dedicated component, which encapsulates the logic for compiling the project, running its tests, and
generating coverage reports. Adding support for new build systems, such as Bazel or Ant, would
require only the implementation of a new handler class and a corresponding container image. This
separation of concerns allows the dataset pipeline to scale to a broader set of languages and tools
in the future without requiring invasive modifications.

A central feature of this phase is the collection of code coverage information. This data is vital
for understanding which parts of the codebase are actually exercised by tests, and in particular,
whether the specific file or method commented on in the review is covered. The coverage analysis
begins by attempting to run the repository’s own coverage configuration, assuming it is already set
up to use JaCoCo, a widely used Java code coverage library. If this initial attempt fails—which is
often the case due to the variability in open-source configurations—the system attempts to inject
JaCoCo manually into the build process.

The injection process is non-trivial. It requires parsing the build file, inserting the necessary
plugin configurations, and re-running the test suite under the modified setup. Care is taken to
preserve the original content of the build file and to restore it in the event of a failure. If the
coverage generation succeeds, the resulting reports are parsed and analyzed to determine coverage at
the file level.

One of the major challenges in this stage stems from the structure of large repositories, many of
which are organized as multi-module or multi-project builds. In such cases, the directory layout is
often arbitrary, with multiple loosely connected subprojects coexisting in a single codebase. This
makes it difficult to automatically associate a given file with the coverage report that pertains to
it. The pipeline addresses this by using the fully qualified name of the Java class—derived from its
package declaration and filename—and searching for it across all available coverage reports. If a
match is found, the file is considered covered.

However, this approach has its limitations. It does not guarantee that the coverage report in which
the class appears is actually the one relevant to the subproject the file belongs to. Given the wide
variety of repository structures and the lack of standardized conventions, reliably linking coverage
data to the correct module or context is extremely difficult to automate. As a result, some
ambiguity remains, and users are encouraged to manually verify the results in edge cases where
coverage attribution may be unclear.

Despite these challenges, the build, test, and coverage phase serves a critical role in the dataset
construction pipeline. It not only ensures that the code under review is functional and testable,
but also provides valuable metadata about how thoroughly it is exercised by tests. This information
is later used to assess the relevance and impact of reviewer comments, particularly in tasks where
test coverage is a factor in determining the usefulness or appropriateness of a suggested change.

\subsection{Manual Selection}
\label{sec:manual-selection}

While the construction of the dataset is driven by automation to ensure scale and consistency, there
remain several aspects that fundamentally depend on human judgment. One of the most important among
these is the annotation of intent and follow-through: determining whether a reviewer’s comment
suggests a change, and whether that change was subsequently implemented in the pull request.

The first layer of this process involves evaluating the nature of the review comment itself. Not all
comments are meant to trigger code modifications. Many comments are purely informational , offering
optional improvements or clarifications that do not warrant actual changes. Some comments are not
directly related to the current pull request but are instead left as notes for future work. These
typically refer to changes that should be made after the branch has been merged with another one,
but which cannot be carried out within the scope of the current pull request due to technical
constraints. Others might highlight a section of code without proposing any specific action. In
order to ensure that the dataset captures meaningful reviewer-author interactions, it is necessary
to distinguish between comments that suggest actionable modifications and those that do not.

A particularly nuanced category of comments consists of those framed as questions. While at first
glance they may appear less directive than imperative statements, questions often carry implicit
suggestions. These can be roughly divided into two main types. The first are rhetorical questions,
which are generally straightforward to interpret. These tend to imply strong disapproval or an
obvious recommendation, masked in the form of a question. For example, a reviewer might write,
\textit{``Can you move this declaration down closer to where it's used?''} or \textit{``Isn't this
redundant with the previous check?''} In most cases, the intent behind such comments is clear: they
point out something that should be removed, refactored, or reconsidered.

The second type of question-based comment is more difficult to classify. These are genuine
inquiries, often posed in good faith, reflecting uncertainty or opening a discussion. They may
express doubt about a design choice, inquire about alternative implementations, or question whether
a particular solution addresses the intended problem. For instance, a reviewer might ask,
\textit{“Would it make sense to use a stream here instead of a loop?”} or \textit{“What happens if
this input is null?”} The challenge with these types of comments is that they can be interpreted in
multiple ways. Some reviewers may expect an immediate change, while others are simply requesting
clarification. In many cases, the boundary between suggestion and inquiry is blurry, and
classification depends on context — including the tone of the comment, the review history, and even
the norms of the repository or organization. As a result, the decision to treat such a comment as a
change suggestion often comes down to the judgment of the person performing the manual annotation.

Once a comment is identified as suggesting a change, the next step is to verify whether the change
was actually implemented. This involves inspecting the commits made after the comment was posted and
analyzing the diffs introduced. However, this is not as straightforward as it may seem. In modern
development workflows, especially those influenced by continuous integration and continuous
deployment (CI/CD) practices, it is common for the pull request branch to be updated with changes
from another branch, such as \texttt{main} or \texttt{dev}, before it is merged. This is often done
to resolve merge conflicts, bring in recent bug fixes, or ensure compatibility with the latest
version of the codebase. While such merges are necessary from an engineering perspective, they
introduce significant noise from the point of view of dataset construction.

When another branch is merged into the pull request, it can bring in a large number of changes that
are unrelated to the comment or even to the pull request itself. As a result, the diffs that appear
after the comment may include modifications that are completely irrelevant to the interaction being
studied. This creates a challenge: how can we isolate the changes that were made in response to the
comment from the rest?

To address this issue, the system incorporates a manual selection step where the user is shown the
diffs produced after the comment and is asked to select which hunks (blocks of code changes) are
relevant to the comment. This allows for precise filtering of the changes and helps construct a
cleaner, more focused dataset where only the diffs that represent a response to the review comment
are preserved.

In some cases, the situation is further complicated by the proximity of changes. Even when
irrelevant changes originate from merges or unrelated commits, they may occur in the same file or
even in adjacent lines to the relevant ones. This results in hunks that contain both the actual
response to the comment and unrelated code. Because the standard diff format groups nearby changes
into a single hunk, it becomes impossible to separate them without manual intervention. To deal with
such situations, the system provides the ability to edit the hunks directly. Users can manually trim
the diff to retain only the portions that directly address the reviewer’s feedback, discarding the
unrelated ones. This kind of fine-grained control is essential for preserving the integrity and
specificity of the dataset.

By performing this dual-level manual selection — first identifying whether a comment suggests a
change, and then isolating the relevant diff hunks — the dataset maintains a high degree of
fidelity. It captures meaningful reviewer-author interactions and filters out unrelated noise
introduced by collaborative development practices. This ensures that the final data is not only
accurate but also truly representative of how developers respond to feedback during the code review
process.


\subsection{Paraphrases generation}
\subsection{Dataset Schema \& Serialization}
\subsection{Dataset Statistics}
\subsection{Summary \& Deliverables}

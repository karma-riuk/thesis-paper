% !TEX root = ../main.tex
\section{CRAB: Code Review Automated Benchmark}

\subsection{Overview}

In this section, we describe the construction of a high-quality Java-only dataset of code-review
triplets. Each triplet comprises three elements: the code snapshot before the review, the natural
language review comment and the revised code after that change has been applied. Our work supports
two complementary benchmarks: the first is review comment generation, asks a model to predict the
review comment given the pre-review code; the second is code refinement, a strict subset of the
first, asks a model to generate the revised code that implements the suggestion.


We focused on Java projects and aimed for high quality over quantity. That means we only kept
examples that are self-contained, testable, and likely to be meaningful. As a starting point, we
used the \textit{GHS} dataset and tool introduced by Dabić \etal~\cite{Dabic:msr2021data}, which
supports project sampling for Mining Software Repositories (MSR) studies by providing rich metadata
for over 700,000 GitHub repositories. Their platform made it possible to filter for Java
repositories with relevant properties like number of commits, license, and recent activity.

This section explains how we constructed the dataset, how we defined the two benchmark tasks
(comment generation and code refinement), and how we filtered and validated the data to ensure it’s
useful for evaluating future models.


\subsection{Data Source}

\subsubsection{Repository Sampling}

We began by querying the \textit{GHS} dataset~\cite{Dabic:msr2021data}, a large-scale repository
mining platform that indexes metadata for over 700,000 GitHub projects. Using the web interface at
\url{https://seart-ghs.si.usi.ch}, we applied two simple filters:

\begin{itemize}
	\item The repository must have had at least 100 pull requests.
	\item The date of the last commit must fall within the year 2025.
\end{itemize}

These criteria ensured that we targeted active, collaborative projects likely to have meaningful
code review activity. We exported the matching repository list in CSV format and used it as input
for our processing pipeline.

We applied minimal filtering beyond that. A small number of repositories were manually excluded
during processing, mostly due to extreme build times or authentication issues.

\subsubsection{Pull Request Selection}

For each selected repository, we searched for pull requests that reflect a clean and minimal review
interaction. Specifically, we looked for PRs that:

\begin{itemize}
	\item Were successfully merged into the main branch.
	\item Contain either one review comment or two, where the second is a reply from the PR author.
\end{itemize}

The goal is to capture cases where a reviewer leaves a single comment and the author either does not
respond in writing, or replies directly within the thread — which we interpret as an acknowledgment
of the feedback. This keeps the setup simple and avoids ambiguous review discussions.

We made this design choice because we realized early on that it would be difficult to create clean
triplets from pull requests with multiple comments. While we can always locate where each comment
appears in the code, it's much harder to untangle which subsequent code changes correspond to which
comment in an automatic way. This makes it challenging to extract multiple, independent \subCode,
\revComment and \revCode triplets from a single pull request. By focusing on PRs with only one
comment (plus an optional reply), we ensure that each PR maps to a single, self-contained instance.
This keeps the data interpretable and makes the benchmark more reliable.

Of course, even in these single-comment PRs, not all follow-up commits are guaranteed to directly
address the reviewer’s feedback. We handle these exceptions through manual validation, as detailed
later in Section~\ref{sec:manual-selection}.

\subsection{Automated Extraction Pipeline}

To process the selected repositories and extract clean code-review triplets, we built a fully
automated pipeline that runs each pull request through a series of steps. The pipeline handles
repository cloning, commit checkout, diff extraction, file inspection, compilation, testing, and
coverage reporting — all inside isolated Docker containers.

Each pull request is treated independently and processed end-to-end. At every step, we log metadata
and errors to track why a PR was accepted or discarded. Figure~\ref{fig:pipeline} gives an overview
of this process. The full list of steps is detailed below, following the same numbering as in the
pipeline diagram.


\begin{figure}[!htbp]
	\centering
	\begin{tikzpicture}[
			node distance=1cm and 1cm,
			every node/.style={font=\small},
			startstop/.style={circle, draw, minimum size=1cm},
			process/.style={rectangle, draw, rounded corners, minimum width=6.5cm, align=center},
			decision/.style={diamond, draw, aspect=2, inner sep=1pt, align=center},
			arrow/.style={->, thick}
		]

		\node (start) [startstop] {Start};
		\node (diffs) [process, below of=start] {1. Extract diffs (before \& after)};
		\node (clone) [process, below of=diffs] {2. Clone repository};
		\node (files) [process, below of=clone] {3. Extract source files (before/after)};
		\node (comment) [process, below of=files] {4. Extract review comment};
		\node (checkout1) [process, below of=comment] {5. Checkout base commit};
		\node (archive1) [process, below of=checkout1] {6. Archive base snapshot};
		\node (checkout2) [process, below of=archive1] {7. Checkout merged commit};
		\node (archive2) [process, below of=checkout2] {8. Archive merged snapshot};

		\node (iscode) [decision, below of=archive2, yshift=-.6cm] {9. Is comment\\code-related?};

		% YES path
		\node (detect) [process, right of=iscode, xshift=7cm] {10. Detect build system (Maven/Gradle)};
		\node (build) [process, below of=detect] {11. Run build in Docker container};
		\node (test) [process, below of=build] {12. Run tests \& extract test results};
		\node (coverage) [process, below of=test] {13. Generate JaCoCo coverage reports};
		\node (record) [process, below of=coverage] {14. Record per-file coverage};

		% compute the midpoint between iscode and record
		\coordinate (mid) at ($ (iscode)!0.5!(record) $);
		\node (save) [process] at ($(mid |- record.south)+(0,-1cm)$) {15. Save entry};

		% arrows - linear path
		\draw[arrow] (start) -- (diffs);
		\draw[arrow] (diffs) -- (clone);
		\draw[arrow] (clone) -- (files);
		\draw[arrow] (files) -- (comment);
		\draw[arrow] (comment) -- (checkout1);
		\draw[arrow] (checkout1) -- (archive1);
		\draw[arrow] (archive1) -- (checkout2);
		\draw[arrow] (checkout2) -- (archive2);
		\draw[arrow] (archive2) -- (iscode);

		% branch arrows
		\draw[arrow] (iscode) -- (detect) node [pos=0.20, above] {yes};
		\draw[arrow] (detect) -- (build);
		\draw[arrow] (build) -- (test);
		\draw[arrow] (test) -- (coverage);
		\draw[arrow] (coverage) -- (record);

		% then re-draw your arrows:
		\draw[arrow] (record) |- (save);
		\draw[arrow] (iscode) |- (save) node [pos=0.025, left] {no};

		\node (end) [startstop, below of = save] {End};
		\draw[arrow] (save) -- (end);

	\end{tikzpicture}
	\caption{Automated pipeline for processing a single pull request.}
	\label{fig:pipeline}
\end{figure}

\begin{enumerate}
    \item \textbf{Extract diffs (before \& after).} Using the GitHub API, we compute the unified
        diffs between the base commit and the first comment, and  the first comment and the
        merged commit.

    \item \textbf{Clone repository.} The GitHub repository is cloned locally, with a check to avoid
        redundant downloads.

    \item \textbf{Extract source files (before/after).} We retrieve the full content of each changed
        file both before and after the merge, and store it in the \path{FileData} structure. If
        GitHub's API fails to deliver, we fall back to reading the file from disk.

    \item \textbf{Extract review comment.} We extract all review comments attached to the pull
        request and filter out those that do not point to a specific file and line range. The GitHub
        API can be inconsistent when it comes to providing line-level comment metadata, which is a
        known issue. To avoid uncertainty and reduce the amount of manual inspection required later,
        we exclude any comments that are missing this information.

    \item \textbf{Checkout base commit.} We check out the commit at the tip of the base branch
        before the pull request was opened. This gives us the clean version of the code before any
        of the proposed changes were introduced.

    \item \textbf{Archive base snapshot.} This version of the code is archived, creating a
        reproducible “before” state for future testing or manual inspection. It can also be used as
        context for a model that requires it.

    \item \textbf{Checkout merged commit.} We then check out the final merged commit of the pull
        request. This is the “after” state, assumed to reflect the author’s response to the review.

    \item \textbf{Archive merged snapshot.} Similar to step 3, we archive the post-merge state to
        preserve the final version of the code.

    \item \textbf{Is comment code-related?} We check whether the comment refers to a Java file. If
        not, there's no point running tests or generating coverage, so we skip straight to storage.

    \item \textbf{Detect build system (Maven/Gradle).} We scan the repository for common build files
        to determine whether to use Maven or Gradle. A specialized handler is then instantiated to
        run the remaining build steps inside Docker.

    \item \textbf{Run build in Docker container.} The project is compiled inside an isolated
        environment.

    \item \textbf{Run tests \& extract test results.} If compilation succeeds, we run the test suite
        and record whether the tests pass cleanly.

    \item \textbf{Generate JaCoCo coverage reports.} We inject a JaCoCo plugin (if necessary) and
        generate line-level test coverage data for each file.

    \item \textbf{Record per-file coverage.} We extract the percentage of covered lines for each
        file touched by the comment and store it in the dataset entry.

    \item \textbf{Save entry.} We save the pull request.
\end{enumerate}

Every pull request that meets our structural criteria (an initial set of commits, a single review
comment, and at least one follow-up commit) is saved to the dataset, regardless of whether the
pipeline completes successfully. Each processing step is allowed to fail independently, and failures
are handled through exception catching. This approach fits our use case well: when a step fails, it
usually means that continuing further would not yield usable results. Rather than discarding these
entries, we record the failure reason directly in the metadata. This makes it possible to gather
statistics about common failure points in the pipeline and refine the system over time. Importantly,
we chose not to remove these PRs entirely because they may still include high-quality review
comments, which can be useful for the comment generation task even if the code does not compile or
pass its tests.


\subsection{Build, Test \& Coverage}

Once valid pull requests have been identified and archived, each repository undergoes a process to
verify its ability to compile, run tests, and provide code coverage information. The first step is
to detect whether the repository contains any tests. This is done by scanning the build
configuration file for references to common testing frameworks (e.g., JUnit, TestNG, Mockito) or
build keywords such as \texttt{testImplementation}. In addition, certain directory structures known
to typically contain tests, like \texttt{src/test/java}, are also considered during detection.

All operations related to compilation, testing, and coverage collection are executed inside Docker
containers to ensure both isolation and reproducibility. The system supports two types of
containers, one for each of the two build systems currently handled: Maven and Gradle. The
architecture is designed to be easily extensible, so support for additional build systems can be
added with minimal effort by implementing a dedicated handler class and corresponding Docker image.

To obtain code coverage information, the system first attempts to run the repository’s existing
configuration using JaCoCo. If this fails, typically because JaCoCo is not already set up in the
project, the system tries to inject JaCoCo configuration into the build file and then re-run the
coverage generation. After coverage reports are generated, they are parsed to extract coverage data
for individual files.

An inherent challenge arises from the fact that many repositories include multiple subprojects,
often organized with arbitrary directory structures. This makes it difficult to associate a given
coverage report with a specific part of the repository. To determine whether the file that received
the comment is covered by tests, the system extracts its fully qualified class name and searches for
its presence in any of the coverage reports. However, it is ultimately up to the user to verify that
the matched coverage file corresponds to the correct subproject, as automated mapping of arbitrary
layouts remains unreliable.

\subsection{Manual selection}
\label{sec:manual-selection}
\subsection{Dataset Schema \& Serialization}
\subsection{Dataset Statistics}
\subsection{Summary \& Deliverables}

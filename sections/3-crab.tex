% !TEX root = ../main.tex
\section{CRAB: Code Review Automated Benchmark}

\subsection{Overview}

In this section, we describe the construction of a high-quality Java-only dataset of code-review
triplets. Each triplet comprises three elements: the code snapshot before the review, the natural
language review comment and the revised code after that change has been applied. Our work supports
two complementary benchmarks: the first is review comment generation, asks a model to predict the
review comment given the pre-review code; the second is code refinement, a strict subset of the
first, asks a model to generate the revised code that implements the suggestion.


We focused on Java projects and aimed for high quality over quantity. That means we only kept
examples that are self-contained, testable, and likely to be meaningful. As a starting point, we
used the \textit{GHS} dataset and tool introduced by Dabić \etal~\cite{Dabic:msr2021data}, which
supports project sampling for Mining Software Repositories (MSR) studies by providing rich metadata
for over 700,000 GitHub repositories. Their platform made it possible to filter for Java
repositories with relevant properties like number of commits, license, and recent activity.

This section explains how we constructed the dataset, how we defined the two benchmark tasks
(comment generation and code refinement), and how we filtered and validated the data to ensure it’s
useful for evaluating future models.


\subsection{Data Source}

\subsubsection{Repository Sampling}

We began by querying the \textit{GHS} dataset~\cite{Dabic:msr2021data}, a large-scale repository
mining platform that indexes metadata for over 700,000 GitHub projects. Using the web interface at
\url{https://seart-ghs.si.usi.ch}, we applied two simple filters:

\begin{itemize}
	\item The repository must have had at least 100 pull requests.
	\item The date of the last commit must fall within the year 2025.
\end{itemize}

These criteria ensured that we targeted active, collaborative projects likely to have meaningful
code review activity. We exported the matching repository list in CSV format and used it as input
for our processing pipeline.

We applied minimal filtering beyond that. A small number of repositories were manually excluded
during processing, mostly due to extreme build times or authentication issues.

\subsubsection{Pull Request Selection}

For each selected repository, we searched for pull requests that reflect a clean and minimal review
interaction. Specifically, we looked for PRs that:

\begin{itemize}
	\item Were successfully merged into the main branch.
	\item Contain either one review comment or two, where the second is a reply from the PR author.
\end{itemize}

The goal is to capture cases where a reviewer leaves a single comment and the author either does not
respond in writing, or replies directly within the thread — which we interpret as an acknowledgment
of the feedback. This keeps the setup simple and avoids ambiguous review discussions.

We made this design choice because we realized early on that it would be difficult to create clean
triplets from pull requests with multiple comments. While we can always locate where each comment
appears in the code, it's much harder to untangle which subsequent code changes correspond to which
comment in an automatic way. This makes it challenging to extract multiple, independent \subCode,
\revComment and \revCode triplets from a single pull request. By focusing on PRs with only one
comment (plus an optional reply), we ensure that each PR maps to a single, self-contained instance.
This keeps the data interpretable and makes the benchmark more reliable.

Of course, even in these single-comment PRs, not all follow-up commits are guaranteed to directly
address the reviewer’s feedback. We handle these exceptions through manual validation, as detailed
later in Section~\ref{sec:manual-selection}.

\subsection{Automated Extraction Pipeline}
\label{sec:pipeline}

To process the selected repositories and extract clean code-review triplets, we built a fully
automated pipeline that runs each pull request through a series of steps. The pipeline handles
repository cloning, commit checkout, diff extraction, file inspection, compilation, testing, and
coverage reporting — all inside isolated Docker containers.

Each pull request is treated independently and processed end-to-end. At every step, we log metadata
and errors to track why a PR was accepted or discarded. Figure~\ref{fig:pipeline} gives an overview
of this process. The full list of steps is detailed below, following the same numbering as in the
pipeline diagram.


\begin{figure}[!htbp]
	\centering
	\begin{tikzpicture}[
			node distance=1cm and 1cm,
			every node/.style={font=\small},
			startstop/.style={circle, draw, minimum size=1cm},
			process/.style={rectangle, draw, rounded corners, minimum width=6.5cm, align=center},
			decision/.style={diamond, draw, aspect=2, inner sep=1pt, align=center},
			arrow/.style={->, thick}
		]

		\node (start) [startstop] {Start};
		\node (diffs) [process, below of=start] {1. Extract diffs (before \& after)};
		\node (clone) [process, below of=diffs] {2. Clone repository};
		\node (files) [process, below of=clone] {3. Extract source files (before/after)};
		\node (comment) [process, below of=files] {4. Extract review comment};
		\node (checkout1) [process, below of=comment] {5. Checkout base commit};
		\node (archive1) [process, below of=checkout1] {6. Archive base snapshot};
		\node (checkout2) [process, below of=archive1] {7. Checkout merged commit};
		\node (archive2) [process, below of=checkout2] {8. Archive merged snapshot};

		\node (iscode) [decision, below of=archive2, yshift=-.6cm] {9. Is comment\\code-related?};

		% YES path
		\node (detect) [process, right of=iscode, xshift=7cm] {10. Detect build system (Maven/Gradle)};
		\node (build) [process, below of=detect] {11. Run build in Docker container};
		\node (test) [process, below of=build] {12. Run tests \& extract test results};
		\node (coverage) [process, below of=test] {13. Generate JaCoCo coverage reports};
		\node (record) [process, below of=coverage] {14. Record per-file coverage};

		% compute the midpoint between iscode and record
		\coordinate (mid) at ($ (iscode)!0.5!(record) $);
		\node (save) [process] at ($(mid |- record.south)+(0,-1cm)$) {15. Save entry};

		% arrows - linear path
		\draw[arrow] (start) -- (diffs);
		\draw[arrow] (diffs) -- (clone);
		\draw[arrow] (clone) -- (files);
		\draw[arrow] (files) -- (comment);
		\draw[arrow] (comment) -- (checkout1);
		\draw[arrow] (checkout1) -- (archive1);
		\draw[arrow] (archive1) -- (checkout2);
		\draw[arrow] (checkout2) -- (archive2);
		\draw[arrow] (archive2) -- (iscode);

		% branch arrows
		\draw[arrow] (iscode) -- (detect) node [pos=0.20, above] {yes};
		\draw[arrow] (detect) -- (build);
		\draw[arrow] (build) -- (test);
		\draw[arrow] (test) -- (coverage);
		\draw[arrow] (coverage) -- (record);

		% then re-draw your arrows:
		\draw[arrow] (record) |- (save);
		\draw[arrow] (iscode) |- (save) node [pos=0.025, left] {no};

		\node (end) [startstop, below of = save] {End};
		\draw[arrow] (save) -- (end);

	\end{tikzpicture}
	\caption{Automated pipeline for processing a single pull request.}
	\label{fig:pipeline}
\end{figure}

\begin{enumerate}
	\item \textbf{Extract diffs (before \& after).} Using the GitHub API, we compute the unified
	      diffs between the base commit and the first comment, and  the first comment and the
	      merged commit.

	\item \textbf{Clone repository.} The GitHub repository is cloned locally, with a check to avoid
	      redundant downloads.

	\item \textbf{Extract source files (before/after).} We retrieve the full content of each changed
	      file both before and after the merge, and store it in the \path{FileData} structure. If
	      GitHub's API fails to deliver, we fall back to reading the file from disk.

	\item \textbf{Extract review comment.} We extract all review comments attached to the pull
	      request and filter out those that do not point to a specific file and line range. The GitHub
	      API can be inconsistent when it comes to providing line-level comment metadata, which is a
	      known issue. To avoid uncertainty and reduce the amount of manual inspection required later,
	      we exclude any comments that are missing this information.

	\item \textbf{Checkout base commit.} We check out the commit at the tip of the base branch
	      before the pull request was opened. This gives us the clean version of the code before any
	      of the proposed changes were introduced.

	\item \textbf{Archive base snapshot.} This version of the code is archived, creating a
	      reproducible “before” state for future testing or manual inspection. It can also be used as
	      context for a model that requires it.

	\item \textbf{Checkout merged commit.} We then check out the final merged commit of the pull
	      request. This is the “after” state, assumed to reflect the author’s response to the review.

	\item \textbf{Archive merged snapshot.} Similar to step 3, we archive the post-merge state to
	      preserve the final version of the code.

	\item \textbf{Is comment code-related?} We check whether the comment refers to a Java file. If
	      not, there's no point running tests or generating coverage, so we skip straight to storage.

	\item \textbf{Detect build system (Maven/Gradle).} We scan the repository for common build files
	      to determine whether to use Maven or Gradle. A specialized handler is then instantiated to
	      run the remaining build steps inside Docker.

	\item \textbf{Run build in Docker container.} The project is compiled inside an isolated
	      environment.

	\item \textbf{Run tests \& extract test results.} If compilation succeeds, we run the test suite
	      and record whether the tests pass cleanly.

	\item \textbf{Generate JaCoCo coverage reports.} We inject a JaCoCo plugin (if necessary) and
	      generate line-level test coverage data for each file.

	\item \textbf{Record per-file coverage.} We extract the percentage of covered lines for each
	      file touched by the comment and store it in the dataset entry.

	\item \textbf{Save entry.} We save the pull request.
\end{enumerate}

Every pull request that meets our structural criteria (an initial set of commits, a single review
comment, and at least one follow-up commit) is saved to the dataset, regardless of whether the
pipeline completes successfully. Each processing step is allowed to fail independently, and failures
are handled through exception catching. This approach fits our use case well: when a step fails, it
usually means that continuing further would not yield usable results. Rather than discarding these
entries, we record the failure reason directly in the metadata. This makes it possible to gather
statistics about common failure points in the pipeline and refine the system over time. Importantly,
we chose not to remove these PRs entirely because they may still include high-quality review
comments, which can be useful for the comment generation task even if the code does not compile or
pass its tests.


\subsection{Build, Test \& Coverage}


\subsubsection{Motivation and Purpose}

Ensuring that each pull request can be built, tested, and instrumented for coverage is a key step in
constructing a dataset that goes beyond static code analysis. This phase is not just a validation
step for technical completeness, it also lays the groundwork for more robust evaluation. The ability
to compile and execute the code opens up the possibility of behavior-based assessment, which is
critical when evaluating code refinement models.

In contrast to similarity-based metrics such as CodeBLEU, which penalize variations in syntax even
when semantics are preserved, test-based evaluation allows for multiple valid implementations to be
accepted as long as they do not introduce regressions. While passing tests do not guarantee the
correctness of a change, they do strongly suggest that the change is not invalid. Failing tests, on
the other hand, clearly indicate broken behavior. This framing enables more realistic and flexible
model evaluation, better aligned with real-world development practices.

\subsubsection{Test Detection}

The first step is to determine whether the repository contains any form of automated
testing. This is accomplished by scanning the build configuration files for known testing libraries
(e.g., JUnit, TestNG, Mockito) and build system keywords such as \path{testImplementation} or
\path{functionalTests}. In addition to static analysis of the build file, the system also
inspects conventional test directories, such as \path{src/test/java} or \path{test/}, which
frequently contain unit or integration tests. This dual approach increases the reliability of test
detection across a broad set of repository structures.

\subsubsection{Execution Environment}

To ensure reproducibility, consistency, and isolation from the host system, all compilation and test
operations are executed within Docker containers. Two container environments are maintained,
corresponding to the two supported Java build systems: Maven and Gradle. Each container includes the
necessary tools, dependencies, and environment settings to execute a typical Java project. This
ensures that the results are not affected by host-specific variations, such as Java versions, OS
configuration, or missing libraries.

The architecture is designed with extensibility in mind. Build system support is modularized so that
new systems (such as Ant or Bazel) can be integrated by adding new handler classes and corresponding
container definitions. This separation of concerns facilitates future expansion to accommodate more
diverse types of software projects. Moreover, because the interface between the build system logic
and the rest of the pipeline is language agnostic, it is hypothetically straightforward to extend
support beyond Java and handle projects written in entirely different programming languages,
provided suitable build and test tooling exists for them.

\subsubsection{Coverage Generation}

Once a repository has been successfully built and tested, the next objective is to collect code
coverage data. This is primarily done using JaCoCo, a widely adopted tool for Java coverage
reporting. If the project is already configured with JaCoCo, the system attempts to run its existing
configuration directly. If this fails, typically due to the absence of the plugin, the system attempts
to inject JaCoCo manually into the build file, modifying the configuration to include the required
setup for coverage generation.

Care is taken to preserve the integrity of the build system: the original configuration is backed up
and restored if the injection fails. When successful, the injected configuration allows for the
generation of XML coverage reports, which are then parsed to extract coverage percentages for
individual files.

\subsubsection{Handling Multi-Project Repositories}

A major challenge arises when dealing with large repositories that are organized into multiple
subprojects. These may be loosely coupled, deeply nested, or follow custom directory conventions.
Because of this structural diversity, it is difficult to reliably map a given Java file to a
specific coverage report. To approximate this mapping, the system extracts the fully qualified class
name of each commented file and searches for it across all available coverage files. If found, the
file is marked as covered.

This approach, while pragmatic, is not foolproof. It does not guarantee that the coverage report
belongs to the same subproject as the file in question. Given the variability in repository layout,
automatic resolution of this ambiguity is not feasible. Users are therefore advised to manually
verify coverage associations in cases where accuracy is critical.

\subsubsection{Enabling Flexible Evaluation}

By ensuring that the code in each pull request is buildable and testable, the dataset allows for a
richer model evaluation framework. Unlike static metrics that enforce a rigid similarity standard,
test-based validation supports functional correctness. This means that a model can propose diverse
implementations in response to reviewer comments, as long as the resulting code passes all tests. In
effect, this introduces a behavior-first evaluation protocol that aligns more closely with how
developers themselves judge correctness: not by form, but by function.

In summary, this phase adds significant value to the dataset by grounding it in real, executable
code, and by enabling flexible, semantically meaningful evaluation. Despite the technical challenges
involved—such as inconsistent configurations, incomplete test setups, and structural complexity—this
approach provides a much more powerful basis for training and evaluating automated code refinement
systems.

\subsection{Manual Selection}
\label{sec:manual-selection}

While the construction of the dataset is driven by automation to ensure scale and consistency, there
remain several aspects that fundamentally depend on human judgment. One of the most important among
these is the annotation of intent and follow-through: determining whether a reviewer’s comment
suggests a change, and whether that change was subsequently implemented in the pull request.

\subsubsection{Comment Classification}
The first layer of this process involves evaluating the nature of the review comment itself. Not all
comments are meant to trigger code modifications. Many comments are purely informational, offering
optional improvements or clarifications that do not warrant actual changes. Some comments are not
directly related to the current pull request but are instead left as notes for future work. These
typically refer to changes that should be made after the branch has been merged with another one,
but which cannot be carried out within the scope of the current pull request due to technical
constraints. Others might highlight a section of code without proposing any specific action. In
order to ensure that the dataset captures meaningful reviewer-author interactions, it is necessary
to distinguish between comments that suggest actionable modifications and those that do not.

A particularly nuanced category of comments consists of those framed as questions. While at first
glance they may appear less directive than imperative statements, questions often carry implicit
suggestions. These can be roughly divided into two main types. The first are rhetorical questions,
which are generally straightforward to interpret. These tend to imply strong disapproval or an
obvious recommendation, masked in the form of a question. For example, a reviewer might write,
\textit{``Can you move this declaration down closer to where it's used?''} or \textit{``Isn't this
	redundant with the previous check?''} In most cases, the intent behind such comments is clear: they
point out something that should be removed, refactored, or reconsidered.

The second type of question-based comment is more difficult to classify. These are genuine
inquiries, often posed in good faith, reflecting uncertainty or opening a discussion. They may
express doubt about a design choice, inquire about alternative implementations, or question whether
a particular solution addresses the intended problem. For instance, a reviewer might ask,
\textit{“Would it make sense to use a stream here instead of a loop?”} or \textit{“What happens if
	this input is null?”} The challenge with these types of comments is that they can be interpreted in
multiple ways. Some reviewers may expect an immediate change, while others are simply requesting
clarification. In many cases, the boundary between suggestion and inquiry is blurry, and
classification depends on context: including the tone of the comment, the review history, and even
the norms of the repository or organization. As a result, the decision to treat such a comment as a
change suggestion often comes down to the judgment of the person performing the manual annotation.

\subsubsection{Assessing Implementation of the Change}
Once a comment is identified as suggesting a change, the next step is to verify whether the change
was actually implemented. This involves inspecting the commits made after the comment was posted and
analyzing the diffs introduced. However, this is not as straightforward as it may seem. In modern
development workflows, especially those influenced by continuous integration and continuous
deployment (CI/CD) practices, it is common for the pull request branch to be updated with changes
from another branch, such as \texttt{main} or \texttt{dev}, before it is merged. This is often done
to resolve merge conflicts, bring in recent bug fixes, or ensure compatibility with the latest
version of the codebase. While such merges are necessary from an engineering perspective, they
introduce significant noise from the point of view of dataset construction.

When another branch is merged into the pull request, it can bring in a large number of changes that
are unrelated to the comment or even to the pull request itself. As a result, the diffs that appear
after the comment may include modifications that are completely irrelevant to the interaction being
studied. This creates a challenge: how can we isolate the changes that were made in response to the
comment from the rest?


\subsubsection{Selective Diff Curation}

To address this issue, the system incorporates a manual selection step where the user is shown the
diffs produced after the comment and is asked to select which hunks (blocks of code changes) are
relevant to the comment. This allows for precise filtering of the changes and helps construct a
cleaner, more focused dataset where only the diffs that represent a response to the review comment
are preserved.

In some cases, the situation is further complicated by the proximity of changes. Even when
irrelevant changes originate from merges or unrelated commits, they may occur in the same file or
even in adjacent lines to the relevant ones. This results in hunks that contain both the actual
response to the comment and unrelated code. Because the standard diff format groups nearby changes
into a single hunk, it becomes impossible to separate them without manual intervention. To deal with
such situations, the system provides the ability to edit the hunks directly. Users can manually trim
the diff to retain only the portions that directly address the reviewer’s feedback, discarding the
unrelated ones. This kind of fine-grained control is essential for preserving the integrity and
specificity of the dataset.

By performing this dual-level manual selection — first identifying whether a comment suggests a
change, and then isolating the relevant diff hunks — the dataset maintains a high degree of
fidelity. It captures meaningful reviewer-author interactions and filters out unrelated noise
introduced by collaborative development practices. This ensures that the final data is not only
accurate but also truly representative of how developers respond to feedback during the code review
process.

\subsubsection{Manual Selection as a Design Choice}

The decision to include a manual selection step was not an afterthought, but a deliberate design
choice. Prior work has shown that fully automated dataset construction methods often introduce a
significant amount of noise—whether due to misclassified comments, ambiguous code changes, or
incidental modifications unrelated to reviewer feedback. Rather than aiming for scale at the expense
of quality, we chose to prioritize precision and relevance. By hand-picking the instances we deemed
meaningful and representative, we ensured that the dataset would reflect true reviewer-author
interactions, with clearly identifiable suggestions and corresponding responses. This curated
approach allows for more reliable evaluation of models in tasks that require nuanced understanding
of code review dynamics.

\subsection{Paraphrases generation}
\subsection{Dataset Schema \& Serialization}
\subsection{Dataset Statistics}
\subsection{Summary \& Deliverables}

% !TEX root = ../main.tex
\section{Introduction}
{\color{red} Readapt}
Code review is a cornerstone of modern software engineering, serving to enhance
code quality, maintainability, and collaboration among developers. Despite its critical
role, the process is expensive and labor-intensive, which has led to the development
of automated solutions to support key aspects, such as generating reviewer-like
comments identifying potential issues and refining the code in response to those comments.
While promising steps have been made using deep learning models, the lack of robust
benchmarks has hindered their reliable evaluation.
% particularly for tasks like review comment generation and code refinement.

Existing datasets, in fact, are automatically built from real code reviews
performed in open-source projects: they automatically try to identify and
extract the code before the review (\subCode), the reviewers' comments
(\revComment), and the code that implements those comments (\revCode). However,
this process is not entirely reliable. For example, several instances extracted
in this way have revised code that does not implement the associated comments
(reviewers' suggestions may be discussed elsewhere and ultimately not accepted)
or contain comments that do not accurately describe the problem found (such as
comments that simply refer to previous comments, e.g., "same here").

Furthermore, the metrics available today only allow a direct syntactic comparison with targets,
considering all those predictions that use different tokens than the target as wrong. For instance,
a comment generated by a model that expresses the same concept as the target but using
different words will be considered wrong. Likewise, the generated revised code that implements
the suggestions contained in the comment in an alternative way to the target will be considered
incorrect even if semantically correct.
Such inconsistencies and limitations obscure the real capabilities of the models.

%Although efforts to clean these datasets exist, the presence of problematic instances remains substantial.
% Consequently, evaluating models using these datasets often produces misleading
% results and impedes meaningful comparisons between approaches.

This thesis will try to address these challenges by introducing a high-quality
benchmark designed to evaluate code review automation approaches.
Specifically, it focuses on two critical tasks: (1) code quality check, \ie
generating natural language review comments for a given piece of code that emulate
human reviewers' comments by identifying problems in the code and making suggestions
for improvement, and (2) revised code generation, \ie generating a piece of code that
implements the reviewers' suggestions for a given piece of code.

The proposed benchmark will consist of meticulously curated triplets (\subCode)
where \revComment will be validated to highlight specific issues in \subCode and
\revCode will be validated to comprehensively address these issues.

To account for natural language variability, reviewers' comments will be accompanied by
alternative paraphrases that maintain their main intent; while to consider as correct also
those alternative code implementations but semantically equivalent to the target, code tests will
be included. These enhancements aim to set a new standard for assessing code review
automation tools by providing consistent and contextually meaningful data.

Furthermore, another goal of this thesis is the development of a web-based platform for researchers to
evaluate their models against the benchmark. By focusing on the Java
programming language and aiming to compile 500 rigorously validated triplets,
this work aspires to provide a reliable foundation for empirical evaluations,
ultimately advancing the state of automated code review.

\subsection{Thesis structure}

{\color{red} TODO}

% A simple bullet list saying: In Chapter 2, we present the state of the art in the field of ….. In Chapter 3….

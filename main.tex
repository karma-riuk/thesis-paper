\documentclass{si-msc-proposal}
\usepackage{hyperref}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{url}
\usepackage{tabularx}
\usepackage[section]{easy-todo}
\usepackage[section]{placeins}  % for \FloatBarrier

\usepackage[export]{adjustbox}

\usepackage{float}
\usepackage{minted}

\setminted{
    fontsize=\small,
    autogobble,
    samepage,
}


\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
  
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,calc}


\author{Arnaud Fauconnet}
\newcommand{\subCode}{\textit{submitted\_code}\xspace}
\newcommand{\revComment}{\textit{reviewer\_comment}\xspace}
\newcommand{\revCode}{\textit{revised\_code}\xspace}

\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\etal}{\emph{et~al.}\xspace}

%\usepackage{biblatex}
%\addbibresource{biblio.bib}

\title{CRAB: Code Review Automation Benchmark}

\subtitle{Advisor: Prof. Grabriele Bavota\\Co-advisor: Dr. Rosalia Tufano}

\abstract{
Code review is an essential practice in software development, enhancing code quality and
maintainability, but it can be resource-intensive and, as a consequence, expensive. To reduce these
costs, researchers have developed code review automation tools that leverage deep learning models to
assist with tasks such as comment generation and code refinement. Comment generation aims at
generating reviewer-like comments that point out potential issues in the code, while the code
refinement task automatically generates an updated version of a given code implementing the reviewer
suggestions. While these tools show great potential, assessing their effectiveness is far from
trivial. Usually, the data on which these models are evaluated (and trained) are triplets composed
by the code submitted for review, reviewers' comments, and the code addressing the reviewers'
comments (revised code). This data is collected automatically from open-source repositories and can
feature noisy instances. A common example of a noisy instance for the code refinement task concerns
a scenario in which the revised code does not address the reviewers' comment, but implements other
changes. In such cases, even if the model predicts the correct code change for the given reviewer
feedback, it is evaluated against a wrong target. Although several researchers have made efforts to
clean the datasets, recent work revealed that problematic instances still make up a significant
portion of the data. To address these issues, we propose the first high-quality benchmark for code
review automation. This benchmark consists of carefully curated \subCode, \revComment, \revCode
triplets, where each comment explicitly points out to an issue in the \subCode and is validated to
ensure that the \revCode accurately implements the requested modification by implementing rigorous
checks to confirm the relevance of comments and the correctness of code refinements. Also, the
benchmark accounts for the possibility that the same quality issue can be expressed with different
wording (important for the ``comment generation'' task) and that the same reviewer's comment can be
addressed with semantically equivalent implementations (important for the ``code refinement'').
Additionally, we created a platform where users can submit model predictions and receive evaluations
with our benchmark.
}

\begin{document}

\maketitle

\input{sections/1-intro}
\input{sections/2-state-of-the-art}
\input{sections/3-crab}
\input{sections/4-webapp}
\input{sections/5-conclusion}


\newpage
\bibliographystyle{unsrt}
\bibliography{biblio}

\end{document}

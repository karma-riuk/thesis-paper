\documentclass{si-msc-proposal}
\usepackage{hyperref}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{url}

\author{Arnaud Fauconnet}
\newcommand{\subCode}{\textit{submitted\_code}\xspace}
\newcommand{\revComment}{\textit{reviewer\_comment}\xspace}
\newcommand{\revCode}{\textit{revised\_code}\xspace}

\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\etal}{\emph{et~al.}\xspace}

%\usepackage{biblatex}
%\addbibresource{biblio.bib}

\title{CRAB: Code Review Automation Benchmark}

\subtitle{Advisor: Prof. Grabriele Bavota\\Co-advisor: Dr. Rosalia Tufano}

\abstract{
    {\color{red} Readapt}
Code review is an essential practice in software development, enhancing code
quality and maintainability, but it can be resource-intensive and, as a consequence, expensive. To reduce these
costs, researchers have developed code review automation tools that leverage
deep learning models to assist with tasks such as code quality check and code
refinement. Code quality check aims at generating reviewer-like comments that
point out potential issues in the code, while the code refinement task automatically generates
an updated version of a given code implementing the reviewer suggestions.
While these tools show great potential, assessing their effectiveness is far from trivial. 
Usually, the data on which these models are evaluated (and trained) are triplets 
composed by the code submitted for review, reviewers' comments, and the code addressing the reviewers' comments (revised code). This data is collected automatically
from open-source repositories and can feature noisy instances. A common example of a noisy instance for the code refinement task concerns a scenario in which the revised code does not address the reviewers' comment, but implements other changes. 
In such cases, even if the model predicts the correct code change for the given reviewer feedback, it is evaluated
against a wrong target. Although several researchers have made efforts to
clean the datasets, recent work revealed that problematic instances still make
up a significant portion of the data. %As a result, evaluating models on such data does not accurately represent their true capabilities.
To address these issues, we aim to propose the first high-quality benchmark for
code review automation. This benchmark will consist of carefully curated
\subCode, \revComment, \revCode triplets, where each comment will explicitly point out
%'before-comment-after' triplets, where each comment will explicitly address a
to an issue in the \subCode and is validated to ensure that the \revCode
accurately implements the requested modification by implementing
rigorous checks to confirm the relevance of comments and the correctness of code
refinements. Also, the benchmark will account for the possibility that the same quality issue can be expressed with different wording (important for the ``quality check'' task) and that the same reviewer's comment can be addressed with semantically equivalent implementations (important for the ``code refinement'').
%The triplets will be
%classified based on the contextual information required to address the comment,
%such as method-only, file-only, project-only or external-dependencies.
Additionally, we will create a platform where users can submit model
predictions and receive evaluations with our benchmark.%, setting a new standard for assessing code review tools. Our focus is on Java, with a goal of compiling 500 validated triplets.
}

\begin{document}

\maketitle

\input{sections/1-intro}
\input{sections/2-state-of-the-art}
\input{sections/3-crab}
\input{sections/4-webapp}
\input{sections/5-conclusion}


\newpage
\bibliographystyle{unsrt}
\bibliography{biblio}

\end{document}
